# Assignment 1: Classifier By Hand


In this assignment, you learn step by step how to code a binary classifier by hand! 

Don't worry, it will be guided through.

## Introduction: calorimeter showers
A calorimeter in the context of experimental particle physics is a sub-detector aiming at measuring the energy of incoming particles. At CERN Large Hadron Collider, the giant multipurpose detectors [ATLAS](https://atlas.cern/Discover/Detector/Calorimeter) and CMS are both equipped with electromagnetic and hadronic calorimeters. The electronic calorimeter, as its name indicates, is measuring the energy of incoming electrons. It is a destructive method: the energetic electron entering the calorimeter will interact with its dense material via the electromagnetic force. It eventually results in the generation of a shower of particles (electromagnetic shower), with a characteristic average depth and width. The depth is along the direction of the incoming particle and the width is the dimension perpendicular to it.

Problem? There can be noissy signals in electromagnetic calorimeters that are generated by hadrons, not electrons. 

Your mission is to help physicists by coding a classifier to select electron-showers (signal) from hadron-showers (background). 

To this end, you are given a dataset of shower characterists from previous measurments where the incoming particle was known. The main features are the depth and the width. 

```{figure} ../images/a01_showers.png
---
  name: a01_showers
  width: 60%
---
Visualization of an electron shower (left) and hadron shower (right).  
 <sub>From [ResearchGate](https://www.researchgate.net/figure/The-different-character-of-electromagneticgamma-and-hadronic-showers-19_fig2_270824497)</sub>
```

```{figure} ../images/a01_showers_distribs.png
---
  name: a01_showers_distribs
  width: 100%
---
Hadron showers are on average longer in the direction of the incoming hadron (depth) and large in the transverse direction (width).  
 <sub>Plot made using dataset.</sub>
```
````{margin}
```{admonition} Dataset
Find the repository on GDrive [here](https://drive.google.com/drive/folders/1hVq0gnDeQvQ7dIvl0bPTyJn9DGMscqwc?usp=sharing).
```
````
## 1. Get the Data
Download the dataset and put it on your GDrive. Open a new Jupyter-notebook. To mount your drive:
```python
from google.colab import drive
drive.mount('/content/gdrive')
```

__1.1 Get and load the data__  
Read the dataset into a dataframe `df`. What are the columns? Which column stores the labels (targets)?

__1.2 How many samples are there?__  

## 2. Feature Scaling

__2.1 Explain with math__  
Before coding, explain using the mathematical expressions seen in Lecture 3 on Logistic Regression why the gradient descent is likely to diverge if features are not rescaled.

_Hints:_  
_Recall that the $\boldsymbol{\theta}$ parameters are usually initialized with random numbers from 0 and 1 (hint again: positive)._  
_Recall the shape of the sigmoid and what it would imply on the cost function, especially for background samples._

__2.2 Standarization__  
Create for each input feature an extra column in the dataframe to rescale it to a distribution of zero-mean and unit-variance. To see statistical information on a dataframe, a convenient method is:
```python
df.describe()
```
We will take $x_1$ and $x_2$ in the order of the dataframe's columns. By searching in the documentation for methods retrieving the mean and standard deviation, complete the following code:
```python
MEAN_X1  =
SIGMA_X1 =   
MEAN_X2  =
SIGMA_X2 =  

df['shower_depth_scaled'] = 
df['shower_width_scaled'] = 
```

_Hint: Recall {prf:ref}`defStandardization` and the equation to scale a feature according to the standardization method._

Check your results by calling `df.describe()` on the updated dataframe.

## 3. Data Prep
Let's make the dataset ready for the classifier. As seen in class, the hypothesis function in the linear assumption is of the form:
```{math}
:label: h_theta_lin_sum_2
  h_\theta(\boldsymbol{x^{(i)}}) = \sum_{j=0}^n \theta_j x^{(i)}_j =  x^{(i)} \theta^{\; T}
```
With 2 input features, there are two parameters to optimize for each feature and the intercept term $\theta_0$. To perform the dot product above, let's add to the dataframe a column:

__3.1 Adding x0 column__  
Add a column `x0` to the dataframe `df` with ones.

__3.2 Matrix X__  
Create a new dataframe `X` that contain the `x0` column and the columns of the two __scaled__ input features.

__3.3 Labels to binary__  
The target column contains alphabetical labels. Create a dataframe `y` containing 1 if the sample is an electron shower and 0 if it is a hadron one.  

_Hint: you can first create an extra column full of 0, then apply a filter using the `.loc` property from DataFrame. It can be in the form:_
```python
df.loc[ CONDITION, COLUMN_TO_UPDATE] = VALUE
```
Read the [pandas documentation on `.loc`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.loc.html). 

__3.4 Vector y__  
Extract from the dataframe the `y` column with the binary labels.

## 4. DataFrames to Numpy
The inputs are almost ready, yet there are still some steps. As we saw in Lecture 3 in Part {ref}`class:algs:perf`, we need to split the dataset in a training and a testing sets. We will use the very convenient `train_test_split` method from Sciki-Learn. Then, we convert the resulting dataframes to NumPy arrays. This Python library is the standard in data science. In this assignment, you will manipulate NumPy arrays and build the foundations you need to master the programming aspect of machine learning.

Copy this code into your notebook:

```python
from sklearn.model_selection import train_test_split

X_train_df, X_test_df, y_train_df, y_test_df = train_test_split( X, y, test_size=0.2, random_state=42)

X_train = X_train_df.to_numpy() ; y_train = y_train_df.to_numpy()
X_test  = X_test_df.to_numpy()  ; y_test  = y_test_df.to_numpy()
```

__4.1 Shapes__  
Show the dimensions of the four NumPy arrays using the `.shape` property. Comment the numbers with respect to the notations defined in class. Does it make sense?

__4.2 Test size__  
Looking at the shapes, explain what `test_size` represents.


## 5. Useful Functions
We saw a lot of functions in the lectures. We will code this functions as 'code block' to make the code more modular and easier to read.




