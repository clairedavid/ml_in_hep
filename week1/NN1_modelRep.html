
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

    <title>Model Representation &#8212; Machine Learning in Particle Physics</title>
    
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">
<link href="../_static/styles/pydata-sphinx-theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">

    
  <link rel="stylesheet"
    href="../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css" />
    <link rel="stylesheet" href="../_static/styles/sphinx-book-theme.css?digest=5115cc725059bd94278eecd172e13a965bf8f5a9" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/proof.css" />
    <link rel="stylesheet" type="text/css" href="../_static/design-style.b7bb847fb20b106c3d81b95245e65545.min.css" />
    
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf">

    <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script src="../_static/scripts/sphinx-book-theme.js?digest=9c920249402e914e316237a7dbc6769907cce411"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../_static/togglebutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../_static/design-tabs.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Activation Functions" href="NN1_activationF.html" />
    <link rel="prev" title="Motivations" href="NN1_motivations.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="None">
    

    <!-- Google Analytics -->
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="60">
<!-- Checkboxes to toggle the left sidebar -->
<input type="checkbox" class="sidebar-toggle" name="__navigation" id="__navigation" aria-label="Toggle navigation sidebar">
<label class="overlay overlay-navbar" for="__navigation">
    <div class="visually-hidden">Toggle navigation sidebar</div>
</label>
<!-- Checkboxes to toggle the in-page toc -->
<input type="checkbox" class="sidebar-toggle" name="__page-toc" id="__page-toc" aria-label="Toggle in-page Table of Contents">
<label class="overlay overlay-pagetoc" for="__page-toc">
    <div class="visually-hidden">Toggle in-page Table of Contents</div>
</label>
<!-- Headers at the top -->
<div class="announcement header-item noprint"></div>
<div class="header header-item noprint"></div>

    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<!-- Sidebar -->
<div class="bd-sidebar noprint" id="site-navigation">
    <div class="bd-sidebar__content">
        <div class="bd-sidebar__top"><div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="../index.html">
      
        <!-- `logo` is deprecated in Sphinx 4.0, so remove this when we stop supporting 3 -->
        
      
      
      <img src="../_static/logo.png" class="logo" alt="logo">
      
      
      <h1 class="site-logo" id="site-title">Machine Learning in Particle Physics</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../intro.html">
                    Machine Learning in Particle Physics
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  About the course
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../about/learningoutcomes.html">
   Learning outcomes
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../about/jupyter.html">
   JupyterHub for class
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Week 1
 </span>
</p>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="trailer.html">
   1. Course Trailer
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox"/>
  <label for="toctree-checkbox-1">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="trailer_hep.html">
     From detectors to publications
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="trailer_mldef.html">
     What is Machine Learning?
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="trailer_pb.html">
     Which problems does ML solve?
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="warmup_1D.html">
   2. Warm up: Linear Regression
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" type="checkbox"/>
  <label for="toctree-checkbox-2">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="warmup_1D_linRegGD.html">
     Gradient Descent in 1D
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="warmup_1D_linRegGDmulti.html">
     Multivariate linear regression
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="warmup_1D_learningR.html">
     Learning Rate
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="warmup_1D_practice.html">
     Gradient Descent in practice
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="class_algs.html">
   3. Classification algorithms
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-3" name="toctree-checkbox-3" type="checkbox"/>
  <label for="toctree-checkbox-3">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="class_algs_logReg.html">
     Logistic Regression: introduction
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="class_algs_sigmoid.html">
     What is the Sigmoid Function?
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="class_algs_costF.html">
     Cost Function for Classification
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="class_algs_reg.html">
     Regularization
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="class_algs_biasV.html">
     Bias, Variance: how to cope
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="BDTs.html">
   4. Decision Trees and Boosting
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-4" name="toctree-checkbox-4" type="checkbox"/>
  <label for="toctree-checkbox-4">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="BDTs_def.html">
     What are Decision Trees?
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="BDTs_forest.html">
     Ensemble Learning and Random Forests
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="BDTs_boosting.html">
     What is boosting?
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 current active has-children">
  <a class="reference internal" href="NN1.html">
   5. Neural Networks Part I
  </a>
  <input checked="" class="toctree-checkbox" id="toctree-checkbox-5" name="toctree-checkbox-5" type="checkbox"/>
  <label for="toctree-checkbox-5">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul class="current">
   <li class="toctree-l2">
    <a class="reference internal" href="NN1_motivations.html">
     Motivations
    </a>
   </li>
   <li class="toctree-l2 current active">
    <a class="current reference internal" href="#">
     Model Representation
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="NN1_activationF.html">
     Activation Functions
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="NN1_feedforward.html">
     Feedforward propagation
    </a>
   </li>
  </ul>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Week 2
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../week2/NN2.html">
   6. Neural Networks Part II
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-6" name="toctree-checkbox-6" type="checkbox"/>
  <label for="toctree-checkbox-6">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../week2/NN2_costF.html">
     Neural Network Cost Function
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../week2/NN2_backprop.html">
     Backpropagation Algorithm
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../week2/NN2_gradCheck.html">
     Initialization and checks
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../week2/NN2_trainNN.html">
     Let’s train our NN!
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../week2/DL.html">
   7. Towards Deep Learning Models
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-7" name="toctree-checkbox-7" type="checkbox"/>
  <label for="toctree-checkbox-7">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../week2/DL_stochGD.html">
     Stochastic Gradient Descent
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../week2/DL_adaptLR.html">
     Adaptative Learning Rates
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../week2/DL_paramHyper.html">
     Hyperparameters in DL
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../week2/DL_scikitPyT.html">
     Tools: scikit-learn &amp; PyTorch
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../week2/CNN.html">
   8. Convolutional Neural Networks
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-8" name="toctree-checkbox-8" type="checkbox"/>
  <label for="toctree-checkbox-8">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../week2/CNN_architecture.html">
     Architecture of a CNN
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../week2/CNN_hyperparameters.html">
     Hyperparameters
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../week2/CNN_regMethods.html">
     Regularization Methods
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../week2/CNN_neutrinoReco.html">
     Application in Neutrino Physics
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../week2/GNN.html">
   9. Graph Neural Networks
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-9" name="toctree-checkbox-9" type="checkbox"/>
  <label for="toctree-checkbox-9">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../week2/GNN_defGraph.html">
     What is a Graph?
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../week2/GNN_applications.html">
     Applications of GNNs
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../week2/GNN_messagePassing.html">
     Message Passing
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../week2/GNN_GConvoN.html">
     Graph Convolutional Networks
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../week2/review.html">
   10. Review
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-10" name="toctree-checkbox-10" type="checkbox"/>
  <label for="toctree-checkbox-10">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../week2/review_summary.html">
     Summary &amp; key points
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../week2/review_FAQ_projects.html">
     FAQ on Projects
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../week2/review_methodology.html">
     Best Practices
    </a>
   </li>
  </ul>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Week 3
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../week3/unsupervised_learning.html">
   11. Unsupervised learning
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-11" name="toctree-checkbox-11" type="checkbox"/>
  <label for="toctree-checkbox-11">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../week3/unsupervised_learning_motiv.html">
     Motivations
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../week3/unsupervised_learning_relevance.html">
     Relevance and examples
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../week3/unsupervised_learning_k-Means.html">
     k-Means Clustering
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../week3/unsupervised_learning_dimRed.html">
     Dimensionality Reduction
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../week3/PCA_AD.html">
   12. PCA and Anomaly Detection
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-12" name="toctree-checkbox-12" type="checkbox"/>
  <label for="toctree-checkbox-12">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../week3/PCA_AD_PCA.html">
     Principal Component Analysis
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../week3/PCA_AD_autoEncoder.html">
     Autoencoders
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../week3/PCA_AD_VAE_for_AD.html">
     Variational Autoencoder for Anomaly Detection
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../week3/ML_BigData.html">
   13. ML in Big Data &amp; Strategies
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-13" name="toctree-checkbox-13" type="checkbox"/>
  <label for="toctree-checkbox-13">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../week3/ML_BigData_7Vs.html">
     The 7V’s of Big Data
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../week3/ML_BigData_preproc.html">
     Data Preprocessing Pipelines
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../week3/ML_BigData_chooseAlgo.html">
     How to Choose an Optimization Algorithm
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../week3/ethics_exam.html">
   14. Ethics in ML &amp; Exam
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-14" name="toctree-checkbox-14" type="checkbox"/>
  <label for="toctree-checkbox-14">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../week3/ethics_exam_1.html">
     The Ethics in ML
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../week3/ethics_exam_2.html">
     Course Exam
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../week3/projects_outlook.html">
   15. Project presentations &amp; Outlook
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-15" name="toctree-checkbox-15" type="checkbox"/>
  <label for="toctree-checkbox-15">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../week3/projects_outlook_pres.html">
     Project Presentations
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../week3/projects_outlook_today.html">
     Open Questions in ML today
    </a>
   </li>
  </ul>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Tutorials
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../tutorials/t01_python.html">
   1. Python: io, variables &amp; plots
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../tutorials/t02_dataprep_linReg.html">
   2. Data Prep &amp; Linear Regression
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../tutorials/t03_signalDisc_LHC.html">
   3. Classification at the LHC
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../tutorials/t04_BDT_workflow.html">
   4. BDT Implementation &amp; Workflow
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../tutorials/t05_nn.html">
   5. Neural Networks by hand
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../tutorials/t06_scikitPyTorch.html">
   6. Intro to scikit-Learn &amp; PyTorch
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../tutorials/t07_CNN_protoDUNE.html">
   7. CNN in protoDUNE
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../tutorials/t08_GNN_CMS.html">
   8. GNN at the LHC
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../tutorials/t09_k-Means.html">
   9. k-Means for Data Quality Monitoring
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../tutorials/t10_anomaly_det.html">
   10. Anomaly Detection for Long-Lived Particle Searches
  </a>
 </li>
</ul>

    </div>
</nav></div>
        <div class="bd-sidebar__bottom">
             <!-- To handle the deprecated key -->
            
            <div class="navbar_extra_footer">
            Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
            </div>
            
        </div>
    </div>
    <div id="rtd-footer-container"></div>
</div>


          


          
<!-- A tiny helper pixel to detect if we've scrolled -->
<div class="sbt-scroll-pixel-helper"></div>
<!-- Main content -->
<div class="col py-0 content-container">
    
    <div class="header-article row sticky-top noprint">
        



<div class="col py-1 d-flex header-article-main">
    <div class="header-article__left">
        
        <label for="__navigation"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="right"
title="Toggle navigation"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-bars"></i>
  </span>

</label>

        
    </div>
    <div class="header-article__right">
<button onclick="toggleFullScreen()"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="bottom"
title="Fullscreen mode"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>

<div class="menu-dropdown menu-dropdown-repository-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Source repositories">
      <i class="fab fa-github"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="https://github.com/executablebooks/jupyter-book"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Source repository"
>
  

<span class="headerbtn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="headerbtn__text-container">repository</span>
</a>

      </li>
      
      <li>
        <a href="https://github.com/executablebooks/jupyter-book/issues/new?title=Issue%20on%20page%20%2Fweek1/NN1_modelRep.html&body=Your%20issue%20content%20here."
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Open an issue"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="headerbtn__text-container">open issue</span>
</a>

      </li>
      
    </ul>
  </div>
</div>

<div class="menu-dropdown menu-dropdown-download-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Download this page">
      <i class="fas fa-download"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="../_sources/week1/NN1_modelRep.md"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Download source file"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="headerbtn__text-container">.md</span>
</a>

      </li>
      
      <li>
        
<button onclick="printPdf(this)"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="left"
title="Print to PDF"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="headerbtn__text-container">.pdf</span>
</button>

      </li>
      
    </ul>
  </div>
</div>
<label for="__page-toc"
  class="headerbtn headerbtn-page-toc"
  
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-list"></i>
  </span>

</label>

    </div>
</div>

<!-- Table of contents -->
<div class="col-md-3 bd-toc show noprint">
    <div class="tocsection onthispage pt-5 pb-3">
        <i class="fas fa-list"></i> Contents
    </div>
    <nav id="bd-toc-nav" aria-label="Page">
        <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#the-basic-units-neurons">
   The basic units: neurons
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#biological-neuron">
     Biological neuron
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#artificial-neuron">
     Artificial neuron
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#the-perceptron">
     The Perceptron
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#limitation-and-abandon">
     Limitation and… abandon
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#connecting-artificial-neurons">
   Connecting artificial neurons
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#layers">
     Layers
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#deep-or-not-deep-is-that-a-question">
     Deep or not deep: is that a question
    </a>
   </li>
  </ul>
 </li>
</ul>

    </nav>
</div>
    </div>
    <div class="article row">
        <div class="col pl-md-3 pl-lg-5 content-container">
            <!-- Table of contents that is only displayed when printing the page -->
            <div id="jb-print-docs-body" class="onlyprint">
                <h1>Model Representation</h1>
                <!-- Table of contents -->
                <div id="print-main-content">
                    <div id="jb-print-toc">
                        
                        <div>
                            <h2> Contents </h2>
                        </div>
                        <nav aria-label="Page">
                            <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#the-basic-units-neurons">
   The basic units: neurons
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#biological-neuron">
     Biological neuron
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#artificial-neuron">
     Artificial neuron
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#the-perceptron">
     The Perceptron
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#limitation-and-abandon">
     Limitation and… abandon
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#connecting-artificial-neurons">
   Connecting artificial neurons
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#layers">
     Layers
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#deep-or-not-deep-is-that-a-question">
     Deep or not deep: is that a question
    </a>
   </li>
  </ul>
 </li>
</ul>

                        </nav>
                    </div>
                </div>
            </div>
            <main id="main-content" role="main">
                
              <div>
                
  <section class="tex2jax_ignore mathjax_ignore" id="model-representation">
<h1>Model Representation<a class="headerlink" href="#model-representation" title="Permalink to this headline">#</a></h1>
<section id="the-basic-units-neurons">
<h2>The basic units: neurons<a class="headerlink" href="#the-basic-units-neurons" title="Permalink to this headline">#</a></h2>
<section id="biological-neuron">
<h3>Biological neuron<a class="headerlink" href="#biological-neuron" title="Permalink to this headline">#</a></h3>
<figure class="align-default" id="lec05-2-neuron-bio">
<a class="reference internal image-reference" href="../_images/lec05_2_neuron_bio.png"><img alt="../_images/lec05_2_neuron_bio.png" src="../_images/lec05_2_neuron_bio.png" style="width: 80%;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 36 </span><span class="caption-text">. Anatomy of a brain cell, the neuron.<br />
<sub>Source: <a class="reference external" href="https://en.wikipedia.org/wiki/Neuron#/media/File:Blausen_0657_MultipolarNeuron.png">Wikimedia - BruceBlaus</a> </sub></span><a class="headerlink" href="#lec05-2-neuron-bio" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
<p>As neural networks are inspired from biological brain cells, let’s start with an anatomical introduction. Neurons are each made of a cell body and one long extension called axon, which at the end has ramifications to connect to other neurons’ dentritic branches (around the next neuron cell’s body). The information propagates in the form an electric signal along the axon. At the end of the axon’s branches (telodendria) are synaptic terminals that are terminal structures releasing, depending on the electrical impulse, chemical messengers called neurotransmitters. Those transmitters propagate the information chemically to the next neuron.</p>
</section>
<section id="artificial-neuron">
<h3>Artificial neuron<a class="headerlink" href="#artificial-neuron" title="Permalink to this headline">#</a></h3>
<p>The first scientists who started to model how biological neurons might work were the neurophysiologist Warren McCulloch and the mathematician Walter Pitts back in … 1943!</p>
<div class="seealso admonition">
<p class="admonition-title">Learn More</p>
<p>Let’s go to the source!<br />
If you are curious to read the original paper marking the birth of the artificial neuron:<br />
“A logical calculus of the ideas immanent in nervous activity” - <a class="reference external" href="https://link.springer.com/article/10.1007/BF02478259">Link on Springer</a></p>
</div>
<div class="proof definition admonition" id="andef">
<p class="admonition-title"><span class="caption-number">Definition 47 </span></p>
<section class="definition-content" id="proof-content">
<p>An <strong>artificial neuron</strong> is an elementary computational unit of artificial neural networks.</p>
<p>It receives inputs from the dataset or other artificial neurons, combine the information and provide output value(s).</p>
<p>Each input node <span class="math notranslate nohighlight">\(x_j\)</span> is associated with a weight <span class="math notranslate nohighlight">\(w_j\)</span>. In terms of components and notations:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(x_j\)</span>: input nodes</p></li>
<li><p><span class="math notranslate nohighlight">\(w_j\)</span>: weights</p></li>
<li><p><span class="math notranslate nohighlight">\(b\)</span>: bias term</p></li>
<li><p><span class="math notranslate nohighlight">\(\sum\)</span>: weighted sum</p></li>
<li><p><span class="math notranslate nohighlight">\(f\)</span>: activation function</p></li>
</ul>
</section>
</div><figure class="align-default" id="lec05-2-ann">
<a class="reference internal image-reference" href="../_images/lec05_2_ann.png"><img alt="../_images/lec05_2_ann.png" src="../_images/lec05_2_ann.png" style="width: 90%;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 37 </span><span class="caption-text">. Schematics of an artificial neuron. The artificial neuron (center blue) receives <span class="math notranslate nohighlight">\(x_n\)</span> input features and a bias term <span class="math notranslate nohighlight">\(b\)</span>. It computes a weighted sum <span class="math notranslate nohighlight">\(\sum\)</span>. The activation function <span class="math notranslate nohighlight">\(f\)</span> decides if the neuron is activated, or ‘fires’ the information to the output value <span class="math notranslate nohighlight">\(y\)</span>.<br />
<sub>Image from the author</sub></span><a class="headerlink" href="#lec05-2-ann" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The weights of an input node is indicative of the strength of this node.</p>
<p>The bias gives the ability to shift the activation curve up and down. It does not depend on any input.</p>
</div>
<p>Now the maths behind a neuron. Recall the linear regression’s sum for the hypothesis function: <span class="math notranslate nohighlight">\(h_\theta(x^{(i)}) = \sum_{j=0}^n \theta_j x^{(i)}_j\)</span>. We had an intercept term <span class="math notranslate nohighlight">\(\theta_0\)</span> that was multiplied with <span class="math notranslate nohighlight">\(x_0\)</span>, where we assumed by convention that <span class="math notranslate nohighlight">\(x_0 = 1\)</span>. We could write it within the sum as we did previously, but in the literature of Neural Networks, it is usually explicitely written outside of the weighted sum from the input nodes (see the comment in the margin). The <span class="math notranslate nohighlight">\(f\)</span> represents the activation function, defined below. So the output of an artificial neuron is given by this key equation:</p>
<aside class="margin sidebar">
<p class="sidebar-title"></p>
<p>We used to write <span class="math notranslate nohighlight">\(h_\theta(x)\)</span> for the hypothesis function while computing linear and logistic regression algorithms. It is still valid if we consider <span class="math notranslate nohighlight">\(\theta = ( \boldsymbol{w}, b)\)</span>, where <span class="math notranslate nohighlight">\(\boldsymbol{w}\)</span> is a vector of weights <span class="math notranslate nohighlight">\(w_1, w_2, \cdots, w_n\)</span> and <span class="math notranslate nohighlight">\(b\)</span> the bias term (playing the role of <span class="math notranslate nohighlight">\(\theta_0\)</span>). To distinguish notations between regression and neural network, let’s prefer here the notation: <span class="math notranslate nohighlight">\(h_{\boldsymbol{w}, b}(x)\)</span> for the weighted sum.</p>
</aside>
<div class="math notranslate nohighlight" id="equation-aneq">
<span class="eqno">(44)<a class="headerlink" href="#equation-aneq" title="Permalink to this equation">#</a></span>\[y = f\left(\sum_{j=1}^n w_j x_j + b \right)\]</div>
<p>This operation is done for one sample row <span class="math notranslate nohighlight">\(x^{(i)}\)</span>, hence reserving the <span class="math notranslate nohighlight">\(i\)</span> index for the sample instances. The sum is done over the input features <span class="math notranslate nohighlight">\(j\)</span> (columns of the <span class="math notranslate nohighlight">\(X\)</span> input matrix). Here the sum starts at <span class="math notranslate nohighlight">\(j=1\)</span> and not zero as the intercept term is written outside of the sum as <span class="math notranslate nohighlight">\(b\)</span>.</p>
<p>We haven’t seen the activation function. Here is it’s definition:</p>
<div class="proof definition admonition" id="activfuncdef">
<p class="admonition-title"><span class="caption-number">Definition 48 </span></p>
<section class="definition-content" id="proof-content">
<p>An <strong>Activation Function</strong>, also called Transfer Function, is a mathematical operation performed by an artificial neuron on the weighted summed of input nodes (plus the bias node).</p>
<p>As the name indicates, it decides whether the neuron’s input to the network is important or not: returning a non-zero values means the <strong>neuron is “activated”</strong>, or “fired.”</p>
<p>The purpose of the activation function is to introduce <strong>non-linearity</strong> into the output of a neural network.</p>
</section>
</div><p>We will see more in details the types of activation functions in the next section.</p>
<p>Now you may wonder how one can compute complex functions with such a simple operating unit. We will see this once we ‘chain’ neurons in different layers. But before that, can one single neuron still be considered a neural network?</p>
<p>Yes it can! And it has a name: it is called the perceptron.</p>
</section>
<section id="the-perceptron">
<h3>The Perceptron<a class="headerlink" href="#the-perceptron" title="Permalink to this headline">#</a></h3>
<div class="proof definition admonition" id="perceptrondef">
<p class="admonition-title"><span class="caption-number">Definition 49 </span></p>
<section class="definition-content" id="proof-content">
<p>A <strong>Perceptron</strong> is a neural network basine on a single specific artificial neuron called a Threshold Logic Unit (TLU) or Linear Threshold Unit (LTU), where the activation function is a step function.</p>
<p>It computes a weighted sum of real-valued inputs, has a bias term in the form of an extra node and yield the output:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*}
h_{\vec{w},b} = \text{step} \left( x^T w + b \right)
\end{equation*}\]</div>
<p>A perceptron is thus a linear binary classifier.</p>
</section>
</div><figure class="align-default" id="lec05-2-perceptron">
<a class="reference internal image-reference" href="../_images/lec05_2_perceptron.png"><img alt="../_images/lec05_2_perceptron.png" src="../_images/lec05_2_perceptron.png" style="width: 90%;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 38 </span><span class="caption-text">. Schematics of a perceptron.<br />
<sub>Image from the author</sub></span><a class="headerlink" href="#lec05-2-perceptron" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
<p>The step functions generally employed are the <em>Heaviside</em> or the <em>sign</em> functions:</p>
<div class="math notranslate nohighlight" id="equation-heaviside">
<span class="eqno">(45)<a class="headerlink" href="#equation-heaviside" title="Permalink to this equation">#</a></span>\[\begin{split}\forall \: z \in  \mathbb{R}, \: \: H(z) =
\begin{cases}
\;\;  1 &amp; \text{ if } z \geq  0 \\
\;\;  0 &amp; \text{ if } z &lt; 0
\end{cases}\end{split}\]</div>
<div class="math notranslate nohighlight" id="equation-sign">
<span class="eqno">(46)<a class="headerlink" href="#equation-sign" title="Permalink to this equation">#</a></span>\[\begin{split}\forall \: z \in  \mathbb{R}, \: \: {sign}(z) = 
 \begin{cases}
\;\;  +1 &amp; \text{ if } z&gt;0 \\
\;\;\;   0 &amp; \text{ if } z=0 \\
\;\;  -1 &amp; \text{ if } z&lt;0 
\end{cases}\end{split}\]</div>
<p>If a perceptron would use the sigmoid as activation function, it would be specified as <em>sigmoid perceptron</em>.</p>
<p>The linear and logistic regression, introduced in Lecture 2 and 3 respectively, can be also achieved by this neuronal animal presented just above. Yet there are core differences between the regression algorithms and the perceptron.</p>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>While linear and logistic regression outputs results that directly be converted to a probability value, the perceptron does not output a probability!<br />
It makes predictions based on a hard threshold.</p>
</div>
<p>The perceptron with one logic threshold neuron can, using the step function, split the data into 2 classes depending on its output value exceeding the threshold (predicted class <span class="math notranslate nohighlight">\(y=1\)</span>) or not (<span class="math notranslate nohighlight">\(y=0\)</span>). Think of the perceptron as more a ‘yes’ or ‘no’ system, and logistic regression as “I think it is 67% probable to be the class <span class="math notranslate nohighlight">\(y=1\)</span>.”</p>
<p>With one output node, a perceptron can classify from two classes. A perceptron with <span class="math notranslate nohighlight">\(k\)</span> output nodes can classify from <span class="math notranslate nohighlight">\(k+1\)</span> classes. The extra class being encoded as “all outputs not activated,” i.e. <span class="math notranslate nohighlight">\(y_1 = y_2 = y_k = y_n = 0\)</span>, would me the event belongs to class <span class="math notranslate nohighlight">\((k+1)^\text{th}\)</span>. It is similar in probabilities when we take:</p>
<div class="math notranslate nohighlight">
\[p(N) = 1 - \sum_{i=1}^{N-1} p_i\]</div>
<p>to get the remaining probability in the last outcome possible (but bare in mind that perceptrons do not output probabilities).</p>
</section>
<section id="limitation-and-abandon">
<h3>Limitation and… abandon<a class="headerlink" href="#limitation-and-abandon" title="Permalink to this headline">#</a></h3>
<p>It turned out that perceptrons disappointed researchers. The main limitation: perceptrons can only solve linearly separable data. In other words, it can classify the data if only one can draw a line (or plane with 3 inputs) sparating the two classes. While perceptrons can successfully perform the basic logical computations (also called ‘gates’) <strong>AND</strong>, <strong>OR</strong> and <strong>NOT</strong>, it is incapable of solving the trivial <strong>XOR</strong> problem:</p>
<figure class="align-default" id="lec05-2-andorxor">
<a class="reference internal image-reference" href="../_images/lec05_2_andorxor.png"><img alt="../_images/lec05_2_andorxor.png" src="../_images/lec05_2_andorxor.png" style="width: 100%;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 39 </span><span class="caption-text">. The XOR is an exclusive OR (true if only the two logical inputs are different). As the data is not linearly separable, a perceptron is unable to solve it.<br />
<sub>Image: extsdd.tistory.com</sub></span><a class="headerlink" href="#lec05-2-andorxor" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
<p>Moreover, the perceptron doesn’t scale well with massive datasets. Other weaknesses of Perceptrons were highlighted in the monograph <a class="reference external" href="https://en.wikipedia.org/wiki/Perceptrons_(book)"><em>Perceptrons</em></a> by computer scientists Marvin Minsky and Seymour Papert in 1969. The book sparked a wave of pessimism and long-standing controversy in the Artificial Intelligence community (barely born), to the point that some researchers were so disappointed that they dropped neural networks altogether. This is known as the <a class="reference external" href="https://en.wikipedia.org/wiki/AI_winter">AI Winter</a>. Interest in neural networks was revived only in the mid 80s.</p>
<div class="seealso admonition">
<p class="admonition-title">Exercise</p>
<ol class="simple">
<li><p>What would be the weights <span class="math notranslate nohighlight">\(w= (w_1, w_2)\)</span> and bias <span class="math notranslate nohighlight">\(b\)</span> to encode the <strong>AND</strong> function above? (using the Heaviside step function)</p></li>
<li><p>Same question for the <strong>OR</strong> function.</p></li>
<li><p>Compare your answers with your peers. Are they the same?</p></li>
</ol>
</div>
<div class="tip dropdown admonition">
<p class="admonition-title">Check your answers</p>
<p><strong>Answer 1.</strong><br />
Possible answer:<br />
<span class="math notranslate nohighlight">\(b = -1\)</span><br />
<span class="math notranslate nohighlight">\(w_1 = 0.5\)</span><br />
<span class="math notranslate nohighlight">\(w_2 = 0.5\)</span></p>
<p><strong>Answer 2.</strong><br />
Possible answer:<br />
<span class="math notranslate nohighlight">\(b = -0.5\)</span><br />
<span class="math notranslate nohighlight">\(w_1 = 1\)</span><br />
<span class="math notranslate nohighlight">\(w_2 = 1\)</span></p>
</div>
</section>
</section>
<section id="connecting-artificial-neurons">
<h2>Connecting artificial neurons<a class="headerlink" href="#connecting-artificial-neurons" title="Permalink to this headline">#</a></h2>
<section id="layers">
<h3>Layers<a class="headerlink" href="#layers" title="Permalink to this headline">#</a></h3>
<p>Now that we saw the artificial neuron, let’s see how they can model complex data patterns while connecting them. Alike their organic counterparts where neurons are organized in consecutive layers, artificial neural networks contains different layers:</p>
<div class="proof definition admonition" id="nnlayerdef">
<p class="admonition-title"><span class="caption-number">Definition 50 </span></p>
<section class="definition-content" id="proof-content">
<p>An artificial neural network is composed:</p>
<ul class="simple">
<li><p><strong>input layer</strong>: one entry layer of input nodes, i.e. input features from the dataset</p></li>
<li><p><strong>output layer</strong>: one final layer of output nodes</p></li>
<li><p><strong>hidden layer(s)</strong>: one of more layers of nodes called “activation units”</p></li>
</ul>
<p>Every layer except the output layer include a bias neuron and is fully connected to the next layer.</p>
</section>
</div></section>
<section id="deep-or-not-deep-is-that-a-question">
<h3>Deep or not deep: is that a question<a class="headerlink" href="#deep-or-not-deep-is-that-a-question" title="Permalink to this headline">#</a></h3>
<div class="proof definition admonition" id="definition-4">
<p class="admonition-title"><span class="caption-number">Definition 51 </span></p>
<section class="definition-content" id="proof-content">
<p>The number of hidden layers is an indicator of the <strong>depth</strong> of a neural network.</p>
</section>
</div><p>A ‘minimal’ multilayer neural network would have one input, one hidden and one output layers. The term used is shallow, by opposition to deep. Stricly speaking , or rather historically from the first development in the 1990s, neural networks containing more than one hidden layers are considered deep. Deep Neural Networks (DNN) are also called Deep Nets for short. There is a bit of fuzziness when it comes to what we can consider deep, as nowadays it is common to see dozens of hidden layers.</p>
<figure class="align-default" id="lec05-2-nn-layers">
<a class="reference internal image-reference" href="../_images/lec05_2_nn_layers.png"><img alt="../_images/lec05_2_nn_layers.png" src="../_images/lec05_2_nn_layers.png" style="width: 90%;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 40 </span><span class="caption-text">. Example of a fully connected, feedforward neural network with 2 hidden layers.<br />
Bias neurons are represented in yellow circles with +1.<br />
<sub>Image from the author</sub></span><a class="headerlink" href="#lec05-2-nn-layers" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
<p>[Hebb rule (?) here or later?]</p>
</section>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./week1"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
            </main>
            <footer class="footer-article noprint">
                
    <!-- Previous / next buttons -->
<div class='prev-next-area'>
    <a class='left-prev' id="prev-link" href="NN1_motivations.html" title="previous page">
        <i class="fas fa-angle-left"></i>
        <div class="prev-next-info">
            <p class="prev-next-subtitle">previous</p>
            <p class="prev-next-title">Motivations</p>
        </div>
    </a>
    <a class='right-next' id="next-link" href="NN1_activationF.html" title="next page">
    <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Activation Functions</p>
    </div>
    <i class="fas fa-angle-right"></i>
    </a>
</div>
            </footer>
        </div>
    </div>
    <div class="footer-content row">
        <footer class="col footer"><p>
  
    By Claire David<br/>
  
      &copy; Copyright 2022.<br/>
    <div class="extra_footer">
      <div>
<a href="https://creativecommons.org/licenses/by-nc-sa/4.0/"><img src="https://licensebuttons.net/l/by-nc-sa/3.0/88x31.png"></a>
    All content on this site (unless otherwise specified) is licensed under the <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/">CC BY-NC-SA 4.0 license</a>
</div>

    </div>
</p>
        </footer>
    </div>
    
</div>


      </div>
    </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf"></script>


  </body>
</html>