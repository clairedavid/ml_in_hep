
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

    <title>Multivariate linear regression &#8212; Machine Learning in Particle Physics</title>
    
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">
<link href="../_static/styles/pydata-sphinx-theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">

    
  <link rel="stylesheet"
    href="../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css" />
    <link rel="stylesheet" href="../_static/styles/sphinx-book-theme.css?digest=5115cc725059bd94278eecd172e13a965bf8f5a9" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/proof.css" />
    <link rel="stylesheet" type="text/css" href="../_static/design-style.b7bb847fb20b106c3d81b95245e65545.min.css" />
    
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf">

    <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script src="../_static/scripts/sphinx-book-theme.js?digest=9c920249402e914e316237a7dbc6769907cce411"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../_static/togglebutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../_static/design-tabs.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Learning Rate" href="warmup_1D_learningR.html" />
    <link rel="prev" title="Gradient Descent in 1D" href="warmup_1D_linRegGD.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="None">
    

    <!-- Google Analytics -->
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="60">
<!-- Checkboxes to toggle the left sidebar -->
<input type="checkbox" class="sidebar-toggle" name="__navigation" id="__navigation" aria-label="Toggle navigation sidebar">
<label class="overlay overlay-navbar" for="__navigation">
    <div class="visually-hidden">Toggle navigation sidebar</div>
</label>
<!-- Checkboxes to toggle the in-page toc -->
<input type="checkbox" class="sidebar-toggle" name="__page-toc" id="__page-toc" aria-label="Toggle in-page Table of Contents">
<label class="overlay overlay-pagetoc" for="__page-toc">
    <div class="visually-hidden">Toggle in-page Table of Contents</div>
</label>
<!-- Headers at the top -->
<div class="announcement header-item noprint"></div>
<div class="header header-item noprint"></div>

    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<!-- Sidebar -->
<div class="bd-sidebar noprint" id="site-navigation">
    <div class="bd-sidebar__content">
        <div class="bd-sidebar__top"><div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="../index.html">
      
        <!-- `logo` is deprecated in Sphinx 4.0, so remove this when we stop supporting 3 -->
        
      
      
      <img src="../_static/logo.png" class="logo" alt="logo">
      
      
      <h1 class="site-logo" id="site-title">Machine Learning in Particle Physics</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../intro.html">
                    Machine Learning in Particle Physics
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  About the course
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../about/learningoutcomes.html">
   Learning outcomes
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../about/jupyter.html">
   JupyterHub for class
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Week 1
 </span>
</p>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="trailer.html">
   1. Course Trailer
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox"/>
  <label for="toctree-checkbox-1">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="trailer_hep.html">
     From detectors to publications
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="trailer_mldef.html">
     What is Machine Learning?
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="trailer_pb.html">
     Which problems does ML solve?
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 current active has-children">
  <a class="reference internal" href="warmup_1D.html">
   2. Warm up: Linear Regression
  </a>
  <input checked="" class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" type="checkbox"/>
  <label for="toctree-checkbox-2">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul class="current">
   <li class="toctree-l2">
    <a class="reference internal" href="warmup_1D_linRegGD.html">
     Gradient Descent in 1D
    </a>
   </li>
   <li class="toctree-l2 current active">
    <a class="current reference internal" href="#">
     Multivariate linear regression
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="warmup_1D_learningR.html">
     Learning Rate
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="warmup_1D_practice.html">
     Gradient Descent in practice
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="class_algs.html">
   3. Classification algorithms
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-3" name="toctree-checkbox-3" type="checkbox"/>
  <label for="toctree-checkbox-3">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="class_algs_logReg.html">
     Logistic Regression: introduction
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="class_algs_sigmoid.html">
     What is the Sigmoid Function?
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="class_algs_costF.html">
     Cost Function for Classification
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="class_algs_reg.html">
     Regularization
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="class_algs_biasV.html">
     Bias, Variance: how to cope
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="BDTs.html">
   4. Decision Trees and Boosting
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-4" name="toctree-checkbox-4" type="checkbox"/>
  <label for="toctree-checkbox-4">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="BDTs_def.html">
     What are Decision Trees?
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="BDTs_forest.html">
     Ensemble Learning and Random Forests
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="BDTs_boosting.html">
     What is boosting?
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="review_week1.html">
   5. Review Week 1
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Week 2
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../week2/NN1.html">
   6. Neural Networks Part I
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-5" name="toctree-checkbox-5" type="checkbox"/>
  <label for="toctree-checkbox-5">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../week2/NN1_motivations.html">
     Motivations
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../week2/NN1_modelRep.html">
     Model Representation
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../week2/NN1_activationF.html">
     Activation Functions
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../week2/NN1_feedforward.html">
     Feedforward Propagation
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../week2/NN2.html">
   7. Neural Networks Part II
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-6" name="toctree-checkbox-6" type="checkbox"/>
  <label for="toctree-checkbox-6">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../week2/NN2_costF.html">
     Neural Network Loss Function
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../week2/NN2_init.html">
     Initialization
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../week2/NN2_backprop.html">
     Backpropagation Algorithm
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../week2/DL.html">
   8. Towards Deep Learning Models
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-7" name="toctree-checkbox-7" type="checkbox"/>
  <label for="toctree-checkbox-7">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../week2/DL_stochGD.html">
     Stochastic Gradient Descent
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../week2/DL_adaptLR.html">
     Varying the Learning Rate
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../week2/DL_paramHyper.html">
     Hyperparameters in DL
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../week2/DL_trainNN.html">
     Let’s train our NN!
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../week2/CNN.html">
   9. Convolutional Neural Networks
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-8" name="toctree-checkbox-8" type="checkbox"/>
  <label for="toctree-checkbox-8">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../week2/CNN_architecture.html">
     Architecture of a CNN
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../week2/CNN_hyperparameters.html">
     Hyperparameters
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../week2/CNN_regMethods.html">
     Regularization Methods
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../week2/CNN_neutrinoReco.html">
     Application in Neutrino Physics
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../week2/review_week2.html">
   10. Review Week 2
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Week 3
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../week3/unsupervised_learning.html">
   11. Unsupervised learning
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-9" name="toctree-checkbox-9" type="checkbox"/>
  <label for="toctree-checkbox-9">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../week3/unsupervised_learning_motiv.html">
     Motivations
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../week3/unsupervised_learning_relevance.html">
     Relevance and examples
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../week3/unsupervised_learning_k-Means.html">
     k-Means Clustering
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../week3/unsupervised_learning_dimRed.html">
     Dimensionality Reduction
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../week3/PCA_AD.html">
   12. PCA and Anomaly Detection
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-10" name="toctree-checkbox-10" type="checkbox"/>
  <label for="toctree-checkbox-10">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../week3/PCA_AD_PCA.html">
     Principal Component Analysis
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../week3/PCA_AD_autoEncoder.html">
     Autoencoders
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../week3/PCA_AD_VAE_for_AD.html">
     Variational Autoencoder for Anomaly Detection
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../week3/ML_BigData.html">
   13. ML in Big Data &amp; Strategies
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-11" name="toctree-checkbox-11" type="checkbox"/>
  <label for="toctree-checkbox-11">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../week3/ML_BigData_7Vs.html">
     The 7V’s of Big Data
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../week3/ML_BigData_preproc.html">
     Data Preprocessing Pipelines
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../week3/ML_BigData_chooseAlgo.html">
     How to Choose an Optimization Algorithm
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../week3/ethics_exam.html">
   14. Ethics in ML &amp; Exam
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-12" name="toctree-checkbox-12" type="checkbox"/>
  <label for="toctree-checkbox-12">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../week3/ethics_exam_1.html">
     The Ethics in ML
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../week3/ethics_exam_2.html">
     Course Exam
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../week3/projects_outlook.html">
   15. Project presentations &amp; Outlook
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-13" name="toctree-checkbox-13" type="checkbox"/>
  <label for="toctree-checkbox-13">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../week3/projects_outlook_pres.html">
     Project Presentations
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../week3/projects_outlook_today.html">
     Open Questions in ML today
    </a>
   </li>
  </ul>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Tutorials
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../tutorials/t01_linear_regression.html">
   1. Linear Regression in Python
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../tutorials/t02_forestry.html">
   2. Event Classification with Trees
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../tutorials/t03_nn_by_hand.html">
   3. Neural Network By Hand
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../tutorials/t04_class_game.html">
   4. Classification Contest
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../tutorials/t05_anomaly_detection.html">
   5. Neural Networks by hand
  </a>
 </li>
</ul>

    </div>
</nav></div>
        <div class="bd-sidebar__bottom">
             <!-- To handle the deprecated key -->
            
            <div class="navbar_extra_footer">
            Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
            </div>
            
        </div>
    </div>
    <div id="rtd-footer-container"></div>
</div>


          


          
<!-- A tiny helper pixel to detect if we've scrolled -->
<div class="sbt-scroll-pixel-helper"></div>
<!-- Main content -->
<div class="col py-0 content-container">
    
    <div class="header-article row sticky-top noprint">
        



<div class="col py-1 d-flex header-article-main">
    <div class="header-article__left">
        
        <label for="__navigation"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="right"
title="Toggle navigation"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-bars"></i>
  </span>

</label>

        
    </div>
    <div class="header-article__right">
<button onclick="toggleFullScreen()"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="bottom"
title="Fullscreen mode"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>

<div class="menu-dropdown menu-dropdown-repository-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Source repositories">
      <i class="fab fa-github"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="https://github.com/executablebooks/jupyter-book"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Source repository"
>
  

<span class="headerbtn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="headerbtn__text-container">repository</span>
</a>

      </li>
      
      <li>
        <a href="https://github.com/executablebooks/jupyter-book/issues/new?title=Issue%20on%20page%20%2Fweek1/warmup_1D_linRegGDmulti.html&body=Your%20issue%20content%20here."
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Open an issue"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="headerbtn__text-container">open issue</span>
</a>

      </li>
      
    </ul>
  </div>
</div>

<div class="menu-dropdown menu-dropdown-download-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Download this page">
      <i class="fas fa-download"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="../_sources/week1/warmup_1D_linRegGDmulti.md"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Download source file"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="headerbtn__text-container">.md</span>
</a>

      </li>
      
      <li>
        
<button onclick="printPdf(this)"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="left"
title="Print to PDF"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="headerbtn__text-container">.pdf</span>
</button>

      </li>
      
    </ul>
  </div>
</div>
<label for="__page-toc"
  class="headerbtn headerbtn-page-toc"
  
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-list"></i>
  </span>

</label>

    </div>
</div>

<!-- Table of contents -->
<div class="col-md-3 bd-toc show noprint">
    <div class="tocsection onthispage pt-5 pb-3">
        <i class="fas fa-list"></i> Contents
    </div>
    <nav id="bd-toc-nav" aria-label="Page">
        <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#gradient-descent-in-multilinear-regression">
   Gradient Descent in Multilinear Regression
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#feature-scaling-normalization">
   Feature Scaling &amp; Normalization
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#what-is-feature-scaling">
     What is feature scaling?
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#mean-normalization">
     Mean normalization
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#standardization">
     Standardization
    </a>
   </li>
  </ul>
 </li>
</ul>

    </nav>
</div>
    </div>
    <div class="article row">
        <div class="col pl-md-3 pl-lg-5 content-container">
            <!-- Table of contents that is only displayed when printing the page -->
            <div id="jb-print-docs-body" class="onlyprint">
                <h1>Multivariate linear regression</h1>
                <!-- Table of contents -->
                <div id="print-main-content">
                    <div id="jb-print-toc">
                        
                        <div>
                            <h2> Contents </h2>
                        </div>
                        <nav aria-label="Page">
                            <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#gradient-descent-in-multilinear-regression">
   Gradient Descent in Multilinear Regression
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#feature-scaling-normalization">
   Feature Scaling &amp; Normalization
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#what-is-feature-scaling">
     What is feature scaling?
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#mean-normalization">
     Mean normalization
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#standardization">
     Standardization
    </a>
   </li>
  </ul>
 </li>
</ul>

                        </nav>
                    </div>
                </div>
            </div>
            <main id="main-content" role="main">
                
              <div>
                
  <section class="tex2jax_ignore mathjax_ignore" id="multivariate-linear-regression">
<h1>Multivariate linear regression<a class="headerlink" href="#multivariate-linear-regression" title="Permalink to this headline">#</a></h1>
<p>In machine learning, data samples contain multiple features.</p>
<p>We will generalize our previous definitions or linear regression to additional features. How to see this?
Previously our data where one column vector <span class="math notranslate nohighlight">\(\boldsymbol{x}\)</span> and its associated column vector of targets <span class="math notranslate nohighlight">\(\boldsymbol{y}\)</span>.
Additional input features can be visualized in the form of a table:</p>
<figure class="align-default" id="tbl-df-example" style="width: 300px">
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>x1</th>
      <th>x2</th>
      <th>x3</th>
      <th>x4</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>44.005516</td>
      <td>-0.523302</td>
      <td>2140.907916</td>
      <td>-0.151751</td>
    </tr>
    <tr>
      <th>1</th>
      <td>49.891157</td>
      <td>-1.953834</td>
      <td>1796.558638</td>
      <td>0.135414</td>
    </tr>
    <tr>
      <th>2</th>
      <td>59.313937</td>
      <td>-0.397455</td>
      <td>1995.958257</td>
      <td>0.047144</td>
    </tr>
    <tr>
      <th>3</th>
      <td>49.362458</td>
      <td>-1.807566</td>
      <td>1901.599055</td>
      <td>-0.078977</td>
    </tr>
    <tr>
      <th>4</th>
      <td>53.120494</td>
      <td>-2.805328</td>
      <td>1834.977822</td>
      <td>0.098095</td>
    </tr>
    <tr>
      <th>5</th>
      <td>44.861903</td>
      <td>-1.719951</td>
      <td>1982.062559</td>
      <td>0.040561</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</figure>
<p>The data set can be represented as a 2D array with training examples listed as rows. Here each training example has four input features <span class="math notranslate nohighlight">\(x_1\)</span>, <span class="math notranslate nohighlight">\(x_2\)</span>, <span class="math notranslate nohighlight">\(x_3\)</span> and <span class="math notranslate nohighlight">\(x_4\)</span>. Each column lists a given feature.</p>
<p>In particle physics, input features can be the properties of the particles in a collision event. For instance <span class="math notranslate nohighlight">\(x_1\)</span> could be the electron momentum, <span class="math notranslate nohighlight">\(x_2\)</span> the electron’s azimutal angle of its direction in the detector’s coordinate system, <span class="math notranslate nohighlight">\(x_3\)</span> could be another particle’s momentum, <span class="math notranslate nohighlight">\(x_4\)</span> could be a special angle between two particles.</p>
<p>Again let’s have consistent notations:</p>
<div class="admonition-terminology-and-notation admonition">
<p class="admonition-title">Terminology and Notation </p>
<ul class="simple">
<li><p>The number of training examples is denoted with <span class="math notranslate nohighlight">\(m\)</span></p></li>
<li><p>The number of features is denoted with <span class="math notranslate nohighlight">\(n\)</span><br />
<br></p></li>
<li><p>The row index or rank of a training example is in superscript, from 1 to <span class="math notranslate nohighlight">\(m\)</span></p></li>
<li><p>The feature or column of a training example is in subscript, from 1 to <span class="math notranslate nohighlight">\(n\)</span></p></li>
</ul>
<p><span class="math notranslate nohighlight">\(\Rightarrow\)</span> the value of input feature <span class="math notranslate nohighlight">\(j\)</span> of training example in row <span class="math notranslate nohighlight">\(i\)</span> is</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*}
x_j^{(i)}
\end{equation*}\]</div>
<p>The training examples form a (<span class="math notranslate nohighlight">\(m \times n\)</span>) matrix <span class="math notranslate nohighlight">\(X\)</span>:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*}
X = \begin{pmatrix}
x_1^{(1)} &amp; x_2^{(1)} &amp; \cdots  &amp; x_j^{(1)} &amp; \cdots &amp; x_n^{(1)} \\[2ex]
x_1^{(2)} &amp; x_2^{(2)} &amp; \cdots &amp; x_j^{(2)} &amp; \cdots &amp; x_n^{(2)} \\
\vdots  &amp; \vdots &amp; \ddots  &amp; \vdots &amp;  &amp; \vdots \\
x_1^{(i)} &amp; x_2^{(i)} &amp; \cdots &amp; x_j^{(i)} &amp; \cdots &amp; x_n^{(i)} \\
\vdots &amp; \vdots &amp;  &amp; \vdots &amp; \ddots  &amp; \vdots \\
x_1^{(m)} &amp; x_2^{(m)} &amp; \cdots &amp; x_j^{(m)} &amp; \cdots &amp; x_n^{(m)} \\
\end{pmatrix} 
\end{equation*}\]</div>
</div>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>The <span class="math notranslate nohighlight">\(i^\text{th}\)</span> training sample <span class="math notranslate nohighlight">\(\boldsymbol{x^{(i)}}\)</span> is not a scalar but a row vector of <span class="math notranslate nohighlight">\(n\)</span> elements.</p>
</div>
<p>Our hypothesis function is generalized to the following for a given training example (row <span class="math notranslate nohighlight">\(i\)</span>):</p>
<div class="proof definition admonition" id="hypothesisFunctionMulti">
<p class="admonition-title"><span class="caption-number">Definition 9 </span></p>
<section class="definition-content" id="proof-content">
<p>The <strong>hypothesis</strong> or <strong>mapping function</strong> for linear regression with <span class="math notranslate nohighlight">\(n\)</span> features is:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*}
h_\theta(\boldsymbol{x^{(i)}}) = \theta_0 + \theta_1 x^{(i)}_1 + \theta_2 x^{(i)}_2 + \cdots + \theta_j x^{(i)}_j + \cdots + \theta_n x^{(i)}_n
\end{equation*}\]</div>
</section>
</div><div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>There are <span class="math notranslate nohighlight">\(n+1\)</span> parameters to optimize as we need to add the offset parameter <span class="math notranslate nohighlight">\(\theta_0\)</span>.</p>
</div>
<p>If we set <span class="math notranslate nohighlight">\(x_0^{(i)} = 1\)</span>, we can write the mapping function as a sum. For one training example <span class="math notranslate nohighlight">\(x^{(i)}\)</span>, i.e. a row in the data set:</p>
<div class="math notranslate nohighlight" id="equation-h-theta-lin-sum">
<span class="eqno">(7)<a class="headerlink" href="#equation-h-theta-lin-sum" title="Permalink to this equation">#</a></span>\[  h_\theta(\boldsymbol{x^{(i)}}) = \sum_{j=0}^n \theta_j x^{(i)}_j =  x^{(i)} \theta^{\; T}\]</div>
<p>where <span class="math notranslate nohighlight">\(x^{(i)}\)</span> an <strong>row vector</strong> of <span class="math notranslate nohighlight">\(n+1\)</span> elements, <span class="math notranslate nohighlight">\(x^{(i)} = (x^{(i)}_0, x^{(i)}_1, x^{(i)}_2, \cdots, x^{(i)}_n)\)</span> and <span class="math notranslate nohighlight">\(\theta\)</span> is a <strong>row vector</strong> too of <span class="math notranslate nohighlight">\(n+1\)</span> elements as well, <span class="math notranslate nohighlight">\(\theta = (\theta_0, \theta_1, \cdots, \theta_n)\)</span>. Thus taking the transposed is equivalent to a dot product:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*}
\boldsymbol{x^{(i)}} \boldsymbol{\theta^{\; T}} =  \begin{pmatrix} x^{(i)}_0, x^{(i)}_1, x^{(i)}_2, \cdots, x^{(i)}_n\\\end{pmatrix} 
  \begin{pmatrix} \theta_0 \\\theta_1 \\ \\\vdots \\  \\\theta_n \\\end{pmatrix} = \; \boldsymbol{x^{(i)}} \cdot  \boldsymbol{\theta}
\end{equation*}\]</div>
<section id="gradient-descent-in-multilinear-regression">
<span id="warmup-linregmulti-graddesc"></span><h2>Gradient Descent in Multilinear Regression<a class="headerlink" href="#gradient-descent-in-multilinear-regression" title="Permalink to this headline">#</a></h2>
<p>We will revisit our algorithm to generalize it to <span class="math notranslate nohighlight">\(\theta_n\)</span> parameters:</p>
<div class="proof algorithm admonition" id="GD_algo_multi">
<p class="admonition-title"><span class="caption-number">Algorithm 2 </span> (Gradient Descent for Multivariate Linear Regression)</p>
<section class="algorithm-content" id="proof-content">
<p><strong>Inputs</strong></p>
<ul class="simple">
<li><p>Training data set <span class="math notranslate nohighlight">\(X\)</span> of <span class="math notranslate nohighlight">\(m\)</span> samples with each <span class="math notranslate nohighlight">\(n\)</span> input features, associated with their targets <span class="math notranslate nohighlight">\(y\)</span>:</p></li>
</ul>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*}
X = \begin{pmatrix}
x_1^{(1)} &amp; x_2^{(1)} &amp; \cdots  &amp; x_j^{(1)} &amp; \cdots &amp; x_n^{(1)} \\[2ex]
x_1^{(2)} &amp; x_2^{(2)} &amp; \cdots &amp; x_j^{(2)} &amp; \cdots &amp; x_n^{(2)} \\
\vdots  &amp; \vdots &amp; \ddots  &amp; \vdots &amp;  &amp; \vdots \\
x_1^{(i)} &amp; x_2^{(i)} &amp; \cdots &amp; x_j^{(i)} &amp; \cdots &amp; x_n^{(i)} \\
\vdots &amp; \vdots &amp;  &amp; \vdots &amp; \ddots  &amp; \vdots \\
x_1^{(m)} &amp; x_2^{(m)} &amp; \cdots &amp; x_j^{(m)} &amp; \cdots &amp; x_n^{(m)} \\
\end{pmatrix}  \hspace{10ex}  y = \begin{pmatrix}
y^{(1)} \\[2ex]
y^{(2)} \\[2ex]
\vdots  \\
y^{(i)}\\
\vdots \\[2ex]
y^{(m)}\end{pmatrix}
\end{equation*}\]</div>
<p><strong>Hyperparameters</strong></p>
<ul class="simple">
<li><p>Learning rate <span class="math notranslate nohighlight">\(\alpha\)</span></p></li>
<li><p>Number of epochs <span class="math notranslate nohighlight">\(N\)</span></p></li>
</ul>
<p><strong>Outputs</strong><br />
The optimized values of <span class="math notranslate nohighlight">\(\theta\)</span> parameters, <span class="math notranslate nohighlight">\(\theta_0\)</span>, <span class="math notranslate nohighlight">\(\theta_1\)</span>, … , <span class="math notranslate nohighlight">\(\theta_n\)</span> minimizing <span class="math notranslate nohighlight">\(J(\theta)\)</span>.</p>
<ol>
<li><p><strong>Initialization</strong>: Set values for all <span class="math notranslate nohighlight">\(\theta\)</span> parameters</p></li>
<li><p><strong>Iterate N times or while the exit condition is not met</strong>:</p>
<ol class="simple">
<li><p><strong>Derivatives of the cost function</strong>:<br />
Compute the partial derivatives <span class="math notranslate nohighlight">\(\frac{\partial }{\partial \theta_j} J(\theta)\)</span> for each <span class="math notranslate nohighlight">\(\theta_j\)</span></p></li>
<li><p><strong>Update the parameters</strong>:<br />
Calculate the new parameters according to:</p></li>
</ol>
<div class="math notranslate nohighlight" id="equation-eqgdlincostmulti">
<span class="eqno">(8)<a class="headerlink" href="#equation-eqgdlincostmulti" title="Permalink to this equation">#</a></span>\[\begin{split}\begin{equation*} \\
\theta'_j = \theta_j-\alpha \frac{\partial}{\partial \theta_j} J\left(\theta\right) \;   \;   \;  \;   \;   \; \forall j \in [0..n]\\
\end{equation*}\end{split}\]</div>
<p>         Reassign the new <span class="math notranslate nohighlight">\(\theta\)</span> parameters to prepare for next iteration</p>
</li>
</ol>
<div class="math notranslate nohighlight" id="equation-equpdatethetamulti">
<span class="eqno">(9)<a class="headerlink" href="#equation-equpdatethetamulti" title="Permalink to this equation">#</a></span>\[\begin{split}\begin{equation*}
\theta_j = \theta'_j \;   \;   \;  \;   \;   \; \forall j \in [0..n]\\
\end{equation*}\end{split}\]</div>
<p><strong>Exit conditions</strong></p>
<ul class="simple">
<li><p>After the maximum number of epochs <span class="math notranslate nohighlight">\(N\)</span> is reached</p></li>
<li><p>If all derivatives are zero:</p></li>
</ul>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*}
\frac{\partial}{\partial \theta_j} J\left(\theta\right) = 0 \;   \;   \;  \;   \;   \;    \forall j \in [0..n]
\end{equation*}\]</div>
</section>
</div><p>The partial derivatives of <span class="math notranslate nohighlight">\(J(\theta)\)</span> for each parameter <span class="math notranslate nohighlight">\(\theta_j\)</span> will be of the form:</p>
<div class="math notranslate nohighlight" id="equation-partialdevlinreg">
<span class="eqno">(10)<a class="headerlink" href="#equation-partialdevlinreg" title="Permalink to this equation">#</a></span>\[\frac{\partial}{\partial \theta_j} J(\theta) = - \frac{1}{m} \sum_{i=1}^{m} \left( h_\theta(x^{(i)}) -  y^{(i)}\right) \; x_j^{(i)}\]</div>
</section>
<section id="feature-scaling-normalization">
<span id="warmup-linregmulti-featurescalnorm"></span><h2>Feature Scaling &amp; Normalization<a class="headerlink" href="#feature-scaling-normalization" title="Permalink to this headline">#</a></h2>
<p>In the example of the previous section, the 3D plot of the cost function with respect to the <span class="math notranslate nohighlight">\(\theta\)</span> parameters was not really bowl-shaped but stretched.
This happens when the features are of different ranges. One parameter will have an increment size larger than the other.</p>
<div class="seealso admonition">
<p class="admonition-title">Exercise</p>
<p>If let’s say <span class="math notranslate nohighlight">\(x_1\)</span> has values between <span class="math notranslate nohighlight">\(10 000\)</span> and <span class="math notranslate nohighlight">\(100000\)</span> and <span class="math notranslate nohighlight">\(x_2\)</span> between 1 and 10.</p>
<ol class="simple">
<li><p>Which parameter, <span class="math notranslate nohighlight">\(\theta_1\)</span> or <span class="math notranslate nohighlight">\(\theta_2\)</span> will be updated with a very large or very small step? (independently of the sign). Hint: ask yourself in which direction the contour is going to be stretched.</p></li>
<li><p>What are the consequences on the gradient descent algorithm?</p></li>
</ol>
</div>
<div class="tip dropdown admonition">
<p class="admonition-title">Check your answers</p>
<p><strong>Answer 1.</strong> If <span class="math notranslate nohighlight">\(x_1\)</span> has very large values, the coefficients <span class="math notranslate nohighlight">\(\theta_0\)</span> will have to be very small and tuned in a fine way to not let the cost function explode in very high values. The <span class="math notranslate nohighlight">\(\theta_2\)</span> parameters will not have to be tuned that finely to prevent the cost function from getting big. Thus the contour plot will be stretched along the parameter associated with the features spanning a small range.</p>
<p>The <span class="math notranslate nohighlight">\(\theta\)</span> parameters will descend (or converge) slowly on large ranges and quickly on small ranges, as the figure below shows:</p>
<figure class="align-default" id="withandoutscaling">
<a class="reference internal image-reference" href="../_images/lec02_2_withandoutScaling.png"><img alt="../_images/lec02_2_withandoutScaling.png" src="../_images/lec02_2_withandoutScaling.png" style="width: 100%;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 14 </span><span class="caption-text">. The contour without feature scaling is skewed and takes up an oval shape. If normalized, the path towards the minimum is shorter.<br />
<sub>Source: <a class="reference external" href="https://www.enjoyalgorithms.com/blog/need-of-feature-scaling-in-machine-learning">enjoyalgorithms.com</a> </sub></span><a class="headerlink" href="#withandoutscaling" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
<p><strong>Answer 2.</strong> The difference in range can lead to an ‘overshooting’ of one parameter value to the other side of the slope, thus creating a zig-zag path towards the minimum. It will slow down the learning process, because more steps are needed before converging to the minimum of the cost function. Another consequence is the risk of divergence: as the learning rate <span class="math notranslate nohighlight">\(\alpha\)</span> is the same while updating all parameters, a big jump in one direction can even lead to instability (more on that in the next section).</p>
</div>
<section id="what-is-feature-scaling">
<h3>What is feature scaling?<a class="headerlink" href="#what-is-feature-scaling" title="Permalink to this headline">#</a></h3>
<div class="proof definition admonition" id="defFeatureScaling">
<p class="admonition-title"><span class="caption-number">Definition 10 </span></p>
<section class="definition-content" id="proof-content">
<p><strong>Feature scaling</strong> is a data preparation process consisting of harmonizing the values of the input variables taken as features for a machine learning algorithm.</p>
</section>
</div><p>Feature scaling can be implemented in different ways. We will see the two classics: mean normalization and standardization.</p>
</section>
<section id="mean-normalization">
<h3>Mean normalization<a class="headerlink" href="#mean-normalization" title="Permalink to this headline">#</a></h3>
<div class="proof definition admonition" id="defMeanNormalization">
<p class="admonition-title"><span class="caption-number">Definition 11 </span></p>
<section class="definition-content" id="proof-content">
<p><strong>Mean normalization</strong> consists of calculating the mean <span class="math notranslate nohighlight">\(\mu_j\)</span> of all training examples <span class="math notranslate nohighlight">\(x_j^{(1)}, x_j^{(2)}, \cdots,  x_j^{(m)}\)</span> of a given feature <span class="math notranslate nohighlight">\(j\)</span> and substract it to each example <span class="math notranslate nohighlight">\(x_j^{(i)}\)</span> of that feature:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*}
\left(x^{(i)}_j\right)' = x^{(i)}_j - \mu_j  \;   \;   \;  \;   \;   \;    \forall i \in [1..m]
\end{equation*}\]</div>
<p>It is usual to divide by the range of the features:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*}
\left(x^{(i)}_j\right)' = \frac{x^{(i)}_j - \mu_j}{x_j^\max - x_j^\min} \;   \;   \;  \;   \;   \;    \forall i \in [1..m]
\end{equation*}\]</div>
</section>
</div><p>Consequence: the mean of the new normalized sample collection for that feature - think of it as an extra column in the matrix <span class="math notranslate nohighlight">\(X\)</span> - will be zero. The data distribute the same as before, the values are just representing the <em>distance to the mean</em>.</p>
</section>
<section id="standardization">
<h3>Standardization<a class="headerlink" href="#standardization" title="Permalink to this headline">#</a></h3>
<div class="proof definition admonition" id="defStandardization">
<p class="admonition-title"><span class="caption-number">Definition 12 </span></p>
<section class="definition-content" id="proof-content">
<p>The mean normalization procedure using as denominator the standard deviation <span class="math notranslate nohighlight">\(\sigma_j\)</span> all the samples for this feature is called <strong>Standardization</strong></p>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*}
x^{(i)}_j = \frac{x^{(i)}_j - \mu_j}{\sigma_j} \;   \;   \;  \;   \;   \;    \forall i \in [1..m]
\end{equation*}\]</div>
</section>
</div><p>Consequence: the mean and standard deviation of the new normalized collection of feature <span class="math notranslate nohighlight">\(j\)</span> will be zero and one respectively.</p>
<p><strong>When is feature scaling relevant? When is it not?</strong><br />
It all depends how the data look like. This is why it is important before starting the machinery of (fancy) learning techniques to inspect the data, plot several distributions if possible and dedicate time for the preparedness steps: data cleaning, scaling etc. This is essential for the success of the fitting algorithm.</p>
</section>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./week1"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
            </main>
            <footer class="footer-article noprint">
                
    <!-- Previous / next buttons -->
<div class='prev-next-area'>
    <a class='left-prev' id="prev-link" href="warmup_1D_linRegGD.html" title="previous page">
        <i class="fas fa-angle-left"></i>
        <div class="prev-next-info">
            <p class="prev-next-subtitle">previous</p>
            <p class="prev-next-title">Gradient Descent in 1D</p>
        </div>
    </a>
    <a class='right-next' id="next-link" href="warmup_1D_learningR.html" title="next page">
    <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Learning Rate</p>
    </div>
    <i class="fas fa-angle-right"></i>
    </a>
</div>
            </footer>
        </div>
    </div>
    <div class="footer-content row">
        <footer class="col footer"><p>
  
    By Claire David<br/>
  
      &copy; Copyright 2022.<br/>
    <div class="extra_footer">
      <div>
<a href="https://creativecommons.org/licenses/by-nc-sa/4.0/"><img src="https://licensebuttons.net/l/by-nc-sa/3.0/88x31.png"></a>
    All content on this site (unless otherwise specified) is licensed under the <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/">CC BY-NC-SA 4.0 license</a>
</div>

    </div>
</p>
        </footer>
    </div>
    
</div>


      </div>
    </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf"></script>


  </body>
</html>