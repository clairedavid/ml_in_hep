# Big Data

## The 7 V's of Big Data

The 7 V's of big data are generally considered to be the key characteristics of very large datasets. They describe the challenges and expectations that are presented by big data. While not exactly 'principles', the 7 V's are often used as a framework for understanding and dealing with big data, and helping developers or organizations understand how to best process and analyze it.


## Challenges
A machine learning project is difficult to implement and it is unlikely to go smooth the first time you push on _Run_. Chances are, there are bugs or deeper errors on the algorithm side. When things go wrong, chances are it may be an issue in your algorithm. But it can also come from the data. It is important to know the situations ... 

### Not big enough
For an algorithm to 'learn' as we saw in previous lectures, it is important to a minimum amount of examples. And the minimum quota depends on the task and associated algorith. Simple one will do fine with thousands of training samples. For advanced architectures such as CNN for image recognition, we are talking about millions on examples to deliver satisfying results! Keep in mind these ballparks to know right at start if you have enough data or need to collect more.

### Biases


======= define bias? representative , key in ML: generalise. 
The best algorithms would fail if the input data sample represents a portion of reality. 


### Scarcity



### Quality

====== link with data cleaning / first inspection. Caution with anomaly detection though! 




===============

this is industry no? 
Where to put Nonrepresentative Training Data / Sampling bias .p52 Geron  (nonresponse bias) / Irrelevant Features
? mention that in HEP it's not really an issue? 
Define data scarcity (?)
IDEA: Point to the chapter! 

